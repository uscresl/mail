<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Robot Manipulation from Cross-Morphology Demonstration">
  <meta name="keywords" content="Imitation from Observation, Learning from Demonstration">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Robot Manipulation from Cross-Morphology Demonstration</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>
  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Robot Manipulation from Cross-Morphology Demonstration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.gautamsalhotra.com/">Gautam Salhotra</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://arthurliu.com/">I-Chun Arthur Liu</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="http://robotics.usc.edu/~gaurav/">Gaurav S. Sukhatme</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Robotics Embedded Systems Laboratory (RESL), USC</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><b>Conference on Robot Learning (CoRL) 2023</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=WGSR7HDuHu"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.03833"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon!)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/hero.mp4"
                type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2> -->
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Some Learning from Demonstrations (LfD) methods handle small mismatches in the action spaces of the teacher and student.
            Here we address the case where the teacher's morphology is substantially different from that of the student. 
            Our framework, <b>Morphological Adaptation in Imitation Learning (MAIL)</b>, bridges this gap allowing us to train an agent from demonstrations by other agents with significantly different morphologies.
            MAIL learns from suboptimal demonstrations, so long as they provide <em>some</em> guidance towards a desired solution.
            We demonstrate MAIL on manipulation tasks with rigid and deformable objects including 3D cloth manipulation interacting with rigid obstacles.
            We train a visual control policy for a robot with one end-effector using demonstrations from a simulated agent with two end-effectors.
            MAIL shows up to 24% improvement in a normalized performance metric over LfD and non-LfD baselines.
            It is deployed to a real Franka Panda robot, handles multiple variations in properties for objects (size, rotation, translation), and cloth-specific properties (color, thickness, size, material).
            We show generalizability to morphology adaptation from n-to-m end-effectors, in a rearrangement task executed in simulation and the real world.
          </p>
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/y7IM83ASzvQ?si=munU7x2ALzU0fzhM"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Overview. -->
    <br/>
    <div class="columns is-centered">
      <div class="column is-full-width has-text-justified">
        <h2 class="title is-3">Overview</h2>
        <br/>
        <img src="./static/images/mail-schematic.png" alt="MAIL schematic">
        <p><center>Morphological Adaption in Imitation Learning (MAIL) framework</center></p>
        <br/>
        <p>
          Learning from Demonstration (LfD) is a set of supervised learning methods where a teacher (often, but not always, a human) demonstrates a task, and a student (usually a robot) uses this information to learn to perform the same task. 
          Some LfD methods cope with small morphological mismatches between the teacher and student (<em>e.g.,</em> five-fingered hand to two-fingered gripper). 
          However, they typically fail for a large mismatch (<em>e.g.,</em> bimanual human demonstration to a robot arm with one gripper).
          The key difference is that to reproduce the transition from a demonstration state to the next, no single student action suffices - a sequence of actions may be needed.
        </p>
        <br/>
        <p>
          We propose a framework, Morphological Adaptation in Imitation Learning (MAIL), to bridge this mismatch.
          We focus on cases where the number of end-effectors is different from teacher to student, specifically 3-to-2, 3-to-1, and 2-to-1.
          It does not require demonstrator actions, only the states of the objects in the environment making it potentially useful for a variety of end-effectors (pickers, suction gripper, two-fingered grippers, or even hands).
          It uses trajectory optimization to convert state-based demonstrations into (suboptimal) trajectories in the student's morphology. 
          The optimization uses a learned (forward) dynamics model to trade accuracy for speed, especially useful for tasks with high-dimensional state and observation spaces.
          The trajectories are then used by an LfD method, which is adapted to work with sub-optimal demonstrations and improve upon them by interacting with the environment.
        </p>
      </div>
    </div>
    <!--/ Overview. -->

    <!-- Results. -->
    <br/>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <p>White sphere in simulation indicates end-effector.</p>
        <br/>

        <h3 class="title is-4">Cloth Fold Comparison</h3>
        <div class="content has-text-justified">
          <p>
          The objective is to fold a flattened cloth into half, along an edge, using two end-effectors. The performance metric is the distance of the cloth particles left of the folding line, to those on the right of the folding line.
          </p>
        </div>
        <video controls muted height="100%">
          <source src="./static/videos/cloth_fold_rollouts_reduced.mp4" type="video/mp4">
        </video>
        <br/>

        <br/>
        <h3 class="title is-4">Dry Cloth Comparison</h3>
        <div class="content has-text-justified">
          <p>
          The objective is to pick up a square cloth from the ground and hang it on a plank to dry. The performance metric is the number of cloth particles (in simulation) on either side of the plank and above the ground.
          </p>
        </div>
        <video controls muted height="100%">
          <source src="./static/videos/dry_cloth_rollouts_reduced.mp4" type="video/mp4">
        </video>
        <br/>

        <br/>
        <h3 class="title is-4">Cloth Fold Sim2Real</h3>
        <video controls muted height="100%">
          <source src="./static/videos/cloth_fold_real_reduced.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified">
            <p>
              <ul>
                <li>Zero-shot sim2real.</li>
                <li>Adjusts to cloth size, color, material, thickness, pose.</li>
              </ul>  
            </p>
          </div>
          <br/>

        <br/>
        <h3 class="title is-4">Dry Cloth Sim2Real</h3>
        <video controls muted height="100%">
          <source src="./static/videos/dry_cloth_real_reduced.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified">
            <p>
              <ul>
                <li>Zero-shot sim2real.</li>
                <li>Adjusts to cloth size, color, material, thickness, pose.</li>
                <li>3D cloth manipulation with a rigid obstacle.</li>
              </ul>  
            </p>
          </div>
          <br/>

        <br/>
        <h3 class="title is-4">Generalizing n-to-m end-effector transfers</h3>
        <h4 class="title is-5">Simulation</h4>
        <div class="content has-text-justified">
          <p>
            Examples of generalizing to different n-to-m end-effector transfers:
            <ul>
              <li>3-to-2 transfer.</li>
              <li>3-to-1 transfer.</li>
              <li>2-to-1 transfer, if we use rollouts from m=2 above as teacher demonstrations for student with m=1 end-effector.</li>
            </ul>  
          </p>
        </div>
        <video controls muted height="100%">
          <source src="./static/videos/n-to-m_sim_reduced.mp4" type="video/mp4">
        </video>

        <br/>
        <br/>
        <h4 class="title is-5">Real World</h4>
        <video controls muted height="100%" width="100%">
          <source src="./static/videos/n-to-m_real_reduced.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified">
            <p>
              <ul>
                <li>Zero-shot sim2real.</li>
              </ul>  
            </p>
        </div>
        <br/>

        <br/>
        <h3 class="title is-4">SOTA Performance Comparisons</h3>
        <div class="content has-text-justified">
          <p>
            For each training run, we used the best model in each seed's training run, and evaluated using 100 rollouts across 5 seeds, different from the training seed.
            Bar height denotes the mean, error bars indicate the standard deviation.
            MAIL outperforms all baselines, in some cases by as much as 24%.
          </p>
        </div>
        <img class="center" src="./static/images/SOTA_Boxplot.png" alt="SOTA boxplot">
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{salhotra2023mail,
      title={Learning Robot Manipulation from Cross-Morphology Demonstration},
      author={Gautam Salhotra and I-Chun Arthur Liu and Gaurav S. Sukhatme},
      booktitle={Conference on Robot Learning},
      year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">nerfies.github.io</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
