<!DOCTYPE html>
<html>
    <title>Goal Proximity Imitation Learning</title>

    <meta charset="UTF-8">
    <meta property="og:title" content="Goal Proximity Imitation Learning">
    <meta property="og:description" content="Lee et al. Generalizable Imitation Learning from Observation via Inferring Goal Proximity">
    <meta property="og:url" content="">
    <meta property="og:image" content="">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1 minimum-scale=1.0">

    <link rel="icon" type="image/png" href="img/favicon-32x32.png">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,400,500,700,900" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

    <!-- Showdown -->
    <script src=" https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
    <script src="js/figure-extension.js"></script>

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- WAVE -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

    <!-- Slick -->
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick-theme.css"/>
    <script type="text/javascript" src="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>

    <link rel="stylesheet" href="theme.css">

    <script>
        const classMap = {
            ul: 'browser-default'
        }

        const bindings = Object.keys(classMap)
        .map(key => ({
            type: 'output',
            regex: new RegExp(`<${key}(.*)>`, 'g'),
            replace: `<${key} class="${classMap[key]}" $1>`
        }));

        const converter = new showdown.Converter({
            extensions: [bindings, 'figure']
        });
        converter.setOption('parseImgDimensions', true);
        converter.setOption('tables', true);
        converter.setFlavor('github');

        $("#markdown-body").ready(() => {
            $.get( "content.md", (data) => {
                const content_html = converter.makeHtml(data);
                $("#markdown-body").html(content_html);
            });
        });

    </script>

    <body>
        <!-- Header -->
        <!-- Wide screen -->
        <header class="hd-container w3-container hide-narrow content-center">
            <div class="w3-cell-row" style="width: 90%; margin: auto; max-width: 1600px; margin-top: 80px; margin-bottom: 40px">
                <div class="w3-container w3-cell w3-cell-middle">
                    <div class="title">Generalizable Imitation Learning from Observation via Inferring Goal Proximity</div>
                    <!-- Author -->
                <div class="w3-row-padding">
                    <div class="authorship-container">
                        <ul class="horizontal-list">
                            <li><a href="https://youngwoon.github.io" target="_blank"><i class="far fa-user"></i> Youngwoon Lee<sup>* 1</sup></a></li>
                            <li><a href="http://andrewszot.com/" target="_blank"><i class="far fa-user"></i> Andrew Szot<sup>* 2</sup></a></li>
                            <li><a href="https://shaohua0116.github.io" target="_blank"><i class="far fa-user"></i> Shao-Hua Sun<sup>1</sup></a></li>
                            <li><a href="https://viterbi-web.usc.edu/~limjj/" target="_blank"><i class="far fa-user"></i> Joseph J. Lim<sup>1</sup> </a></li>
                        </ul>
                        <ul class="horizontal-list">
                            <li><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> University of Southern California<sup>1</sup> </a></li>
                            <li><a href="" target="_blank"><i class="fas fa-university"></i> Georgia Institute of Technology<sup>2</sup> </a></li>
                        </ul>
                    </div>
                    </div>
                    <div class="excerpt w3-padding-16" style="width: 80%; max-width: 700px; margin: auto;">
                        Task progress is intuitive and readily available task information that can guide an agent closer to the desired goal. Furthermore, a task progress estimator can generalize to new situations. From this intuition, we propose a simple yet effective imitation learning from observation method for a goal-directed task using a learned goal proximity function as a task progress estimator for better generalization to unseen states and goals. We obtain this goal proximity function from expert demonstrations and online agent experience, and then use the learned goal proximity as a dense reward for policy training. We demonstrate that our proposed method can robustly generalize compared to prior imitation learning methods on a set of goal-directed tasks in navigation, locomotion, and robotic manipulation, even with demonstrations that cover only a part of the states.
                    </div>
                </div>
            </div>
        </header>

        <!-- Narrow screen -->
        <header class="hd-container w3-container hide-wide">
            <div class="w3-row-padding w3-center w3-padding-24">
                <span class="title">Generalizable Imitation Learning from Observation via Inferring Goal Proximity</span>
            </div>
            <div class="w3-row-padding">
                <!-- Author -->
                <div class="authorship-container">
                    <ul class="horizontal-list">
                        <li><a href="https://youngwoon.github.io" target="_blank"><i class="far fa-user"></i> Youngwoon Lee<sup>* 1</sup></a></li>
                        <li><a href="http://andrewszot.com/" target="_blank"><i class="far fa-user"></i> Andrew Szot<sup>* 2</sup></a></li>
                        <li><a href="https://shaohua0116.github.io" target="_blank"><i class="far fa-user"></i> Shao-Hua Sun<sup>1</sup></a></li>
                        <li><a href="https://viterbi-web.usc.edu/~limjj/" target="_blank"><i class="far fa-user"></i> Joseph J. Lim<sup>1</sup> </a></li>
                    </ul>
                    <ul class="horizontal-list">
                        <li><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> University of Southern California<sup>1</sup> </a></li>
                        <li><a href="" target="_blank"><i class="fas fa-university"></i> Georgia Institute of Technology<sup>2</sup> </a></li>
                    </ul>
                </div>

            </div>
            <div class="w3-row-padding"><hr></div>
            <div class="w3-row-padding w3-padding-16">
                <div class="excerpt">
                    Task progress is intuitive and readily available task information that can guide an agent closer to the desired goal. Furthermore, a task progress estimator can generalize to new situations. From this intuition, we propose a simple yet effective imitation learning from observation method for a goal-directed task using a learned goal proximity function as a task progress estimator for better generalization to unseen states and goals. We obtain this goal proximity function from expert demonstrations and online agent experience, and then use the learned goal proximity as a dense reward for policy training. We demonstrate that our proposed method can robustly generalize compared to prior imitation learning methods on a set of goal-directed tasks in navigation, locomotion, and robotic manipulation, even with demonstrations that cover only a part of the states.
                </div>
            </div>
        </header>

        <!-- Main Body -->
        <div class="main-body">
            <div class="w3-container">
                <div class="w3-content" style="max-width:1000px;">
                    <!-- Links -->
                    <div class="link-container">
                        <ul class="horizontal-list">
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-file-alt"></i> <a href="https://openreview.net/forum?id=lp9foO8AFoD" target="_blank"> Paper </a></button></li>
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-code"></i> <a href="https://github.com/clvrai/goal_prox_il" target="_blank"> Code </a></button></li>
                        </ul>
                    </div>
                    <!-- Markdown Body -->
                    <div id="markdown-body"></div>
                </div>
            </div>
        </div>

        <!-- Footer -->
        <footer class="w3-center w3-light-grey w3-padding-32 w3-small">
            <p style="color: grey">
                This work was partially done during Youngwoon Lee worked at NAVER AI Lab as an intern.
                This research is partially supported by the Annenberg Fellowship from USC and the funding from National Science Foundation (NSF NRI-2024768) and NAVER AI Lab.
                We thank members of the USC CLVR lab for constructive feedback. <br/>
                &copy; Copyright 2021, CLVR, USC.
            </p>
        </footer>

    </body>
</html>
